"""Build and refresh the Parquet cache used by the project.

CSV files located under :data:`RAW_DIR` are merged into a single Parquet
file stored at :data:`CACHE`. A :class:`filelock.FileLock` guards the write
operation so parallel processes do not corrupt the cache. Subsequent runs
load this consolidated dataset directly without re-reading the individual
CSV sources.
"""

from pathlib import Path

import pandas as pd
from filelock import FileLock
from loguru import logger

from finansal_analiz_sistemi import config

RAW_DIR = Path("veri/ham")
CACHE = Path(config.PARQUET_CACHE_PATH)
LOCK_FILE = CACHE.with_suffix(".lock")


def build() -> None:
    """Build the Parquet cache from the raw CSV files.

    Reads all CSV files under ``RAW_DIR`` and writes them as a single
    Parquet file to ``CACHE``. When an existing cache is found, the
    function returns without rebuilding.
    """
    with FileLock(str(LOCK_FILE)):
        if CACHE.exists() and CACHE.stat().st_size > 0:
            logger.info("Cache hit, skipping build")
            return
        logger.info("CSV \u279c Parquet \u00f6nbellek olusturuluyorâ€¦")
        dfs = [pd.read_csv(p, parse_dates=["date"]) for p in RAW_DIR.glob("*.csv")]
        if not dfs:
            logger.warning("Ham CSV bulunamadi: %s", RAW_DIR)
            df = pd.DataFrame()
        else:
            df = (
                pd.concat(dfs, ignore_index=True)
                .drop_duplicates(["ticker", "date"])
                .sort_values(["ticker", "date"])
            )
        CACHE.parent.mkdir(parents=True, exist_ok=True)
        df.to_parquet(CACHE, index=False)
        logger.success("Parquet cache yazildi (%d satir)", len(df))
